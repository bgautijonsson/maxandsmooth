% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrartcl}

\usepackage{amsmath,amssymb}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else  
    % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\setlength{\emergencystretch}{3em} % prevent overfull lines
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother

\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother

\ifLuaTeX
  \usepackage{selnolig}  % disable illegal ligatures
\fi
\usepackage{bookmark}

\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same} % disable monospaced font for URLs
\hypersetup{
  pdftitle={Applying Max-and-Smooth to the UKCP data},
  pdfauthor={Brynjólfur Gauti Guðrúnar Jónsson},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Applying Max-and-Smooth to the UKCP data}
\author{Brynjólfur Gauti Guðrúnar Jónsson}
\date{2024-10-10}

\begin{document}
\maketitle


\section{Introduction}\label{introduction}

This document describes the implementation of the Max-and-Smooth
algorithm for fast approximate Bayesian inference in spatial extreme
value analysis of climate projections provided by the UKCP. The
algorithm is specifically applied to Generalized Extreme Value (GEV)
distributions and is implemented in C++ with R interfaces using Rcpp and
RcppEigen.

\subsection{Package Overview}\label{package-overview}

The \texttt{maxandsmooth} R package provides tools for fast approximate
Bayesian inference for spatial GEV models. The core of the package is
implemented in C++ for efficiency, with R wrappers for ease of use.

Key features of the package include:

\begin{itemize}
\tightlist
\item
  Implementation of the Max-and-Smooth algorithm for GEV distributions
\item
  Efficient C++ code using Eigen for linear algebra operations
\item
  Spatial modeling of GEV parameters using Intrinsic Conditional
  Autoregressive (ICAR) priors
\item
  R interface for easy integration into existing extreme value analysis
  workflows
\end{itemize}

\subsection{Algorithm Description}\label{algorithm-description}

The Max-and-Smooth algorithm, as applied to spatial GEV models, consists
of two main steps:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \textbf{Max Step}: Maximum likelihood estimation of GEV parameters at
  each spatial location
\item
  \textbf{Smooth Step}: Spatial smoothing of the maximum likelihood
  estimates using a latent Gaussian field
\end{enumerate}

The algorithm treats the ML estimates as sufficient statistics for a
latent Gaussian field, providing a fast approximation to full Bayesian
inference for spatial extreme value models.

\subsection{Code Structure}\label{code-structure}

The package is organized into several key files:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \texttt{src/max.cpp}: Implements the Max step (maximum likelihood
  estimation for GEV)
\item
  \texttt{src/smooth.cpp}: Implements the Smooth step (latent Gaussian
  field smoothing)
\item
  \texttt{src/maxandsmooth.cpp}: Ties together the Max and Smooth steps
\item
  \texttt{src/gev.cpp}: Implements GEV-specific functions for likelihood
  and gradient calculations
\end{enumerate}

\section{Max Step}\label{max-step}

The Max step involves computing location-wise maximum likelihood
estimates (MLEs) for the GEV model parameters. This step is performed
independently for each location, treating the data as if it were
independent across locations.

\subsection{Data Structure}\label{data-structure}

The input data Y, calculated from the UKCP, is structured as a matrix,
where:

\begin{itemize}
\tightlist
\item
  Rows represent observations of hourly maximum rainfall in yearly
  blocks
\item
  Columns represent different spatial locations over Great Britain.
\end{itemize}

\subsection{Maximum Likelihood
Estimation}\label{maximum-likelihood-estimation}

For each location (column in Y), we compute the MLEs for the three GEV
parameters: location \((\mu)\), scale \((\sigma)\), and shape \((\xi)\).
The log-likelihood function for the GEV distribution at a single
location is:

\[
\ell(\mu, \sigma, \xi | y) = -n\log\sigma - (1+\frac{1}{\xi})\sum_{i=1}^n \log\left(1+\xi\frac{y_i-\mu}{\sigma}\right) - \sum_{i=1}^n \left(1+\xi\frac{y_i-\mu}{\sigma}\right)^{-1/\xi}
\]

where \(n\) is the number of observations at the location.

The MLEs are obtained by maximizing this likelihood function with
respect to the parameters:

\[
(\hat{\mu}_i, \hat{\sigma}_i, \hat{\xi}_i) = \arg\max_{(\mu, \sigma, \xi)} \ell(\mu, \sigma, \xi | Y_i)
\]

where \(Y_i\) is the data for location \(i\).

\subsubsection{Link Functions}\label{link-functions}

Instead of directly maximizing the likelihood with the original
parameters, we transform the parameters using a link function

\[
\left(\psi, \tau, \phi\right) = h(\mu, \sigma, \xi) = \left(\log(\mu), \log(\sigma) - \log(\mu), \text{logit}(\xi)\right)
\]

\subsection{Implementation}\label{implementation}

The maximization is performed using numerical optimization techniques.
In our C++ implementation, we use the following approach:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Parameters are transformed with a link function
\item
  The negative log-likelihood and its gradient are computed on the
  unconstrained scale
\item
  A numerical optimizer is used to find the MLEs as well as the Hessians
  at each location's optimum.
\end{enumerate}

The output of this step includes:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  A vector of parameter estimates, \(\hat\eta\), ordered such that each
  station's location parameter appears first, followed by scale
  parameters, then shape parameters:

  \[
  \hat{\eta} = (\hat{\psi}_1, \ldots, \hat{\psi}_n, \hat{\tau}_1, \ldots, \hat{\tau}_n, \hat{\phi}_1, \ldots, \hat{\phi}_n)^T
  \]

  where \(n\) is the number of stations.
\item
  A precision matrix, \(Q_{\eta y}\), constructed from the negative
  Hessians of the log-likelihood at the MLE estimates. Due to the
  parameter ordering in \(\hat\eta\), \(Q_{\eta y}\) can be described as
  a 3×3 block matrix:

  \[
  Q_{\eta y} = \begin{bmatrix}
  Q_{\psi\psi} & Q_{\psi\tau} & Q_{\psi\phi} \\
  Q_{\tau\psi} & Q_{\tau\tau} & Q_{\tau\phi} \\
  Q_{\phi\psi} & Q_{\psi\tau} & Q_{\phi\phi}
  \end{bmatrix}
  \]

  where each block \(Q_{ij}\) is an \(n \times n\) diagonal matrix. The
  diagonal elements of \(Q_{ii}\) correspond to the negative second
  derivatives of the log-likelihood with respect to the \(i\)-th
  parameter at each station. The off-diagonal blocks \(Q_{ij}\) (where
  \(i \neq j\)) contain the negative mixed partial derivatives of the
  log-likelihood with respect to the \(i\)-th and \(j\)-th parameters.

  For example, the elements of \(Q_{\psi\psi}\) are the conditional
  precisions of the location parameters:

  \[
  \begin{aligned}
  Q_{\psi\psi} &=  \text{diag}\left(\tau^{\psi\psi}_1, \dots, \tau^{\psi\psi}_n\right)\\
  &= \text{diag}\left(-\frac{\partial^2 \ell(Y_1|\psi_1,\tau_1,\phi_1)}{\partial \psi_1^2}, \ldots, -\frac{\partial^2 \ell(Y_n|\psi_n,\tau_n,\phi_n)}{\partial \psi_n^2}\right),
  \end{aligned}
  \]

  and the elements of \(Q_{\mu\xi}\) are the conditional dependencies
  between the location and shape parameters:

  \[
  \begin{aligned}
  Q_{\psi\phi} &= \text{diag}\left(\tau^{\psi\phi}_1, \dots, \tau^{\psi\phi}_n\right) \\
  &= \text{diag}\left(-\frac{\partial^2 \ell(Y_1|\psi_1,\tau_1,\phi_1)}{\partial \psi_1\partial \phi_1}, \ldots, -\frac{\partial^2 \ell(Y_n|\psi_n,\tau_n,\phi_n)}{\partial \psi_n\partial \phi_n}\right)
  \end{aligned}
  \]
\end{enumerate}

This structure reflects the independence assumption between stations in
the Max step, while capturing the parameter dependencies within each
station. The outputs, \(\hat \eta\) and \(Q_{\eta y}\), serve as inputs
into the Smooth step.

\section{Smooth Step}\label{smooth-step}

The Smooth step involves Bayesian inference on a latent Gaussian field,
using the maximum likelihood estimates from the Max step as data.

\subsection{Model Structure}\label{model-structure}

Let \(\eta\) be the latent field of parameters, and \(\hat{\eta}\) be
the maximum likelihood estimates from the Max step. The model is
structured as follows:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Data level: \(\hat{\eta} | \eta \sim N(\eta, Q_{\eta y}^{-1})\)
\item
  Latent level: \(\eta | \tau_\eta \sim N(0, Q_\eta^{-1})\)
\item
  Hyperparameter level: \(\tau_\eta \sim \pi(\tau_\eta)\)
\end{enumerate}

where \(Q_{\eta y}\) is the precision matrix from the Max step, and
\(Q_\eta\) is the precision matrix of the latent field, parameterized by
hyperparameters \(\tau_\eta\).

\subsubsection{Data Level}\label{data-level}

The data level models the relationship between the observed maximum
likelihood estimates \(\hat{\eta}\) and the true latent field \(\eta\):

\[\hat{\eta} | \eta \sim N(\eta, Q_{\eta y}^{-1})\]

Here, \(Q_{\eta y}\) is the precision matrix obtained from the Max step,
representing the uncertainty in the MLE estimates. This can be thought
of as using the outputs of the Max step as sufficient statistics for the
observed data.

\subsubsection{Latent Level}\label{latent-level}

The latent level models the spatial structure of the parameter field: \[
\eta | \tau_\eta \sim N(0, Q_\eta^{-1})
\]

Here, \(Q_\eta\) is a block diagonal precision matrix that encodes the
spatial dependence structure of the latent field. The block diagonal
structure reflects the assumption of independence between different
parameters, while allowing for spatial correlation within each parameter
field. The precision matrix \(Q_\eta\) is constructed as follows:

\[
Q_\eta = \begin{bmatrix}
\tau_\psi Q_{\text{prior}} & 0 & 0 \\
0 & \tau_\tau Q_{\text{prior}} & 0 \\
0 & 0 & \tau_\phi Q_{\text{prior}}
\end{bmatrix}
\]

where

\[
Q_{\text{prior}} = \begin{bmatrix}
2 & -1 \\
-1 & 2 & -1 \\
& -1 & 2 & -1 \\
& & \vdots & \vdots & \vdots \\
& & & -1 & 2 & -1 \\
& & & & -1 & 2 & -1 \\
& & & & & -1 & 2
\end{bmatrix}
\]

and \(\tau_\eta = \left(\tau_\psi, \tau_\tau, \tau_\phi\right)^T\) are
the hyperparameters controlling the strength of spatial dependence for
each parameter.

\subsubsection{Hyperparameter Level}\label{hyperparameter-level}

The hyperparameters \(\tau_\eta\) are assigned prior distributions.

\subsection{MCMC Sampling}\label{mcmc-sampling}

The Smooth step uses Metropolis-Hastings MCMC to sample from this
posterior. The algorithm alternates between:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Proposing new hyperparameters \(\tau_\eta^*\)
\item
  Accepting or rejecting the proposal based on the Metropolis-Hastings
  ratio
\item
  Sampling a new latent field \(\eta^*\) conditional on \(\tau_\eta^*\)
\end{enumerate}

\subsubsection{Proposing new
hyperparameters}\label{proposing-new-hyperparameters}

\subsubsection{Caltulating the Metropolis-Hastings
Ratio}\label{caltulating-the-metropolis-hastings-ratio}

The marginal posterior density of \(\tau_\eta\) given \(\hat \eta\) is
\(\pi(\tau_\eta\vert\hat\eta) \propto \pi(\tau_\eta)\pi(\hat\eta\vert\tau_\eta)\)
and it can be represented as

\[
\pi(\tau_\eta \vert \hat \eta) \propto \pi(\tau_\eta) \frac{\pi(\hat \eta \vert \eta, \tau_\eta)\pi(\eta \vert \tau_\eta)}{\pi(\eta \vert \hat \eta, \tau_\eta)},
\]

using the fact that
\(\pi(\hat\eta \vert \tau_\eta)\pi(\eta\vert\hat\eta, \tau_\eta) = \pi(\hat\eta\vert\eta,\tau_\eta)\pi(\eta\vert\tau_\eta)\).
The densities on the right hand contain \(\eta\), but since the density
is independent of \(\eta\) we can choose any value for \(\eta\)
\emph{(for example 0)} when calculating the marginal posterior of
\(\tau_\eta\). In the following, we will show the densities when
\(\eta = 0\)

\paragraph{\texorpdfstring{\(\pi(\hat \eta \vert \eta, \tau_\eta)\)}{\textbackslash pi(\textbackslash hat \textbackslash eta \textbackslash vert \textbackslash eta, \textbackslash tau\_\textbackslash eta)}}\label{pihat-eta-vert-eta-tau_eta}

This is a normal distribution:

\[
\hat\eta\vert\eta, \tau_\eta \sim \mathrm{N}(\eta, Q_{\eta y}).
\]

Up to a proportionality constant, this evaluates to

\[
\frac12 \left(\log\vert Q_{\eta y}\vert - (\hat \eta - \eta)^TQ_{\eta y}(\hat \eta - \eta)\right)
\]

Since we can choose \(\eta = 0\), we get

\[
\frac12 \left(\log\vert Q_{\eta y}\vert - \hat \eta^TQ_{\eta y}\hat \eta\right),
\]

A value that is completely independent of \(\eta\) and \(\tau_eta\).
This means we can pre-compute it only once and store the values.

\paragraph{\texorpdfstring{\(\pi(\eta \vert \tau_\eta)\)}{\textbackslash pi(\textbackslash eta \textbackslash vert \textbackslash tau\_\textbackslash eta)}}\label{pieta-vert-tau_eta}

\[
\eta \vert \tau_\eta \sim \mathrm{N}(0, Q_\eta)
\]

Up to a proportionality constant, this evaluates to

\[
\frac12\left(\log\vert Q_\eta\vert - \eta^TQ_\eta\eta\right)
\]

Choosing \(\eta = 0\), this simplifies to

\[
\frac12\log\vert Q_\eta\vert = \frac12  \sum_{i=1}^3\log\vert \tau_i Q_\text{prior}\vert =
\frac12  \left(3 \log\vert Q_\text{prior}\vert + \sum_{i=1}^3 n\log \tau_i\right)
\]

We can precompute the determinant of \(Q_\text{prior}\) and perform
these calculations efficiently

\paragraph{\texorpdfstring{\(\pi(\eta \vert \hat \eta, \tau_\eta)\)}{\textbackslash pi(\textbackslash eta \textbackslash vert \textbackslash hat \textbackslash eta, \textbackslash tau\_\textbackslash eta)}}\label{pieta-vert-hat-eta-tau_eta}

\[
\eta \vert \hat\eta, \tau_\eta \sim \mathrm{N}(\mu_\text{post}, Q_\text{post})
\]

where

\[
Q_{\text{post}} = Q_{\eta y} + Q_\eta
\qquad
\text{and}
\qquad
\mu_{\text{post}} = Q_{\text{post}}^{-1}\hat b_{\eta y}.
\]

Up to a proportionality constant, this becomes

\[
\frac12\left(\log\vert Q_\text{post}\vert - (\eta - \mu_\text{post})^TQ_\text{post}(\eta - \mu_\text{post}) \right)
\]

Choosing \(\eta = 0\), we get

\[
\frac12\left(\log\vert Q_\text{post}\vert - \mu_\text{post}^TQ_\text{post}\mu_\text{post} \right)
\]

but

\[
\mu_\text{post}^TQ_\text{post}\mu_\text{post} = (Q_\text{post}^{-1}\hat b_{\eta y})^T Q_\text{post}(Q_\text{post}^{-1}\hat b_{\eta y}) = \hat b_{\eta y}^TQ_\text{post}^{-1}\hat b_{\eta y}
\]

Writing this up using the Cholesky factor, \(L_\text{post}\) of
\(Q_\text{post}\), we get

\[
\hat b_{\eta y}^TQ_\text{post}^{-1}\hat b_{\eta y} = \hat b_{\eta y}^T(L_\text{post} L_\text{post}^T)^{-1}\hat b_{\eta y} = \vert\vert L_\text{post}^{-1}\hat b_{\eta y}\vert\vert
\]

This means that to calculate this density, we must first compute
\(L_\text{post}\), then compute \(\vert\vert y\vert \vert\), where

\[
L_\text{post}y = \hat b_{\eta y}
\]

This can be efficiently computed using the block-Cholesky methods
described in the appendix.

\paragraph{\texorpdfstring{\(\log \pi(\tau_\eta \vert \hat \eta) - \log \pi(\tau_\eta^* \vert \hat \eta)\)}{\textbackslash log \textbackslash pi(\textbackslash tau\_\textbackslash eta \textbackslash vert \textbackslash hat \textbackslash eta) - \textbackslash log \textbackslash pi(\textbackslash tau\_\textbackslash eta\^{}* \textbackslash vert \textbackslash hat \textbackslash eta)}}\label{log-pitau_eta-vert-hat-eta---log-pitau_eta-vert-hat-eta}

In practice, we are not exactly interested in the marginal posterior,
\(\pi(\tau_\eta \vert \hat \eta)\). Rather, we want to know the value of

\[
\begin{aligned}
\log \pi(\tau_\eta \vert \hat \eta) - \log \pi(\tau_\eta^* \vert \hat \eta) &= 
\left( \log\pi(\tau_\eta) - \log\pi(\tau_\eta^*) \right)  \\ 
&+\left( \log\pi(\hat \eta \vert \eta, \tau_\eta) - \log\pi(\hat \eta \vert \eta, \tau_\eta^*) \right) \\
&+ \left( \log\pi(\eta \vert \tau_\eta) - \log \pi(\eta \vert \tau_\eta^*) \right) \\
&+ \left( \log \pi(\eta \vert \hat \eta, \tau_\eta) - \log \pi(\eta \vert \hat \eta, \tau_\eta^*) \right)
\end{aligned}
\]

In particular, since \(\log\pi(\hat \eta \vert \eta, \tau_\eta)\) is
independent of \(\tau_\eta\), this cancels out.

Similarly

\[
\log\pi(\eta \vert \tau_\eta) - \log \pi(\eta \vert \tau_\eta^*) = \frac{n}{2}\sum_{i=1}^3 \log \tau_i - \log \tau_i^*
\]

The conditional posterior of \(\eta | \hat\eta, \tau_\eta\) becomes

\[
\log \pi(\eta \vert \hat \eta, \tau_\eta) - \log \pi(\eta \vert \hat \eta, \tau_\eta^*) = 
\sum_{i=1}^{3n} L_{[i,i] \text{post}} - \sum_{i=1}^{3n}L_{[i,i] \text{post}}^*  - 
\frac12\left( \vert\vert L_\text{post}^{-1}\hat b_{\eta y}\vert\vert - \vert\vert L_\text{post}^{*-1}\hat b_{\eta y}\vert\vert  \right)
\]

Alltogether, this leaves us with:

\[
\begin{aligned}
\log \pi(\tau_\eta \vert \hat \eta) - \log \pi(\tau_\eta^* \vert \hat \eta) &= 
\left( \log\pi(\tau_\eta) - \log\pi(\tau_\eta^*) \right)  \\ 
&+ \frac{n}{2}\sum_{i=1}^3\left( \log \tau_i - \log \tau_i^*\right) \\
&+ \sum_{i=1}^{3n} L_{[i,i] \text{post}} - \sum_{i=1}^{3n}L_{[i,i] \text{post}}^* \\
&- \frac12\left( \vert\vert L_\text{post}^{-1}\hat b_{\eta y}\vert\vert - \vert\vert L_\text{post}^{*-1}\hat b_{\eta y}\vert\vert  \right)
\end{aligned} 
\]

\subsubsection{Latent Field Sampling}\label{latent-field-sampling}

The latent field is sampled from the conditional normal distribution

\[
\eta \vert \hat\eta, \tau_\eta \sim \mathrm{N}(\mu_{\text{post}}, Q_{\text{post}}),
\]

where

\[
Q_{\text{post}} = Q_{\eta y} + Q_\eta
\qquad
\text{and}
\qquad
\mu_{\text{post}} = Q_{\text{post}}^{-1}Q_{\eta y}\hat\eta.
\]

The first thing we can do to make these calculations faster is to
precompute \(\hat b_{\eta y} = Q_{\eta y} \hat \eta\) between the Max
step and the Smooth step, giving us

\[
Q_{\text{post}} = Q_{\eta y} + Q_\eta
\qquad
\text{and}
\qquad
\mu_{\text{post}} = Q_{\text{post}}^{-1}\hat b_{\eta y}.
\]

The most efficient way to sample from this posterior is to calculate the
Cholesky decomposition

\[
Q_{\text{post}} = L_{\text{post}}L_{\text{post}}^T,
\]

and using that to solve for \(\mu_\text{post}\) in

\[
L_{\text{post}}L_{\text{post}}^T \mu_\text{post} = \hat b_{\eta y}.
\]

After that, we sample from the latent field by generating a standard
normal random vector \(z \sim \mathrm N(0, I)\) and using forward
substitution to calculate:

\[
\eta^* = \mu_\text{post} + L_\text{post}^{-1}z
\]

\subsubsection{Algorithm}\label{algorithm}

All in all, this leaves us with the following steps to efficiently draw
from the posterior:

\textbf{Initialization}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Initialize \(\tau_\eta\) and \(\eta\) to sensible values.
\item
  Compute \(L_\text{post}\) using Block-Cholesky algorithm and store it.
\item
  Compute \(y\) by solving \(L_\text{post}y = \hat b_{\eta y}\) and
  store it.
\item
  Compute \(\mu_\text{post}\) by solving
  \(L_\text{post}^T\mu_\text{post} = y\) and store it.
\item
  Compute
  \(\frac12\log\vert Q_\text{post}\vert = \sum_{i=1}^{3n} L_{[i,i] \text{post}}\)
  and store it
\end{enumerate}

\textbf{Sampling}

For each iteration:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  Propose \(\tau_\eta^*\)
\item
  Compute \(L_\text{post}^*\) using Block-Cholesky algorithm and store
  it.
\item
  Compute \(y^*\) by solving \(L_\text{post}^*y = \hat b_{\eta y}\) and
  store it.
\item
  Compute
  \(\frac12\log\vert Q_\text{post}^*\vert = \sum_{i=1}^{3n} L_{[i,i] \text{post}}^*\)
  and store it
\item
  Compute the log-acceptance ratio
  \(\log \pi(\tau_\eta \vert \hat \eta) - \log(\tau_\eta^*\vert \hat \eta)\)
  and accept/reject the proposed \(\tau_\eta^*\)
\item
  If \(\tau_\eta^*\) is accepted:

  \begin{enumerate}
  \def\labelenumii{\roman{enumii}.}
  \tightlist
  \item
    Set \(L_\text{post} = L_\text{post}^*\), \(y = y^*\) and
    \(\frac12\log\vert Q_\text{post}\vert = \frac12\log\vert Q_\text{post}^*\vert\)
  \item
    Compute \(\mu_\text{post}\) by solving
    \(L_\text{post}^T\mu_\text{post} = y\) and store it.
  \item
    Sample from the latent field using the new values of
    \(\mu_\text{post}\) and \(L_\text{post}\)
  \end{enumerate}
\item
  If \(\tau_\eta^*\) is rejected:

  \begin{enumerate}
  \def\labelenumii{\roman{enumii}.}
  \tightlist
  \item
    Sample from the latent field using the stored values of
    \(\mu_\text{post}\) and \(L_\text{post}\)
  \end{enumerate}
\end{enumerate}

\section{Appendix}\label{appendix}

\subsection{\texorpdfstring{Computing
\(L_\text{post}\)}{Computing L\_\textbackslash text\{post\}}}\label{computing-l_textpost}

We first write out \(Q_\text{post}\) as

\[
Q_\text{post} = \begin{bmatrix}
Q_{\psi\psi} + \tau_\psi Q_\text{prior} & Q_{\psi\tau} & Q_{\psi\phi} \\
Q_{\tau\psi} & Q_{\tau\tau} + \tau_\tau Q_\text{prior} & Q_{\tau\phi} \\
Q_{\phi\psi} & Q_{\phi\tau} & Q_{\phi\phi} + \tau_\phi Q_\text{prior},
\end{bmatrix}
\]

where each block on the diagonal has bandwidth 3 and all the
off-diagonal blocks are diagonal matrices. Keep in mind that since the
off-diagonal blocks are diagonal matrices we have that
\(Q_{\psi\tau} = Q_{\tau\psi}\), \(Q_{\psi\phi} = Q_{\phi\psi}\) and
\(Q_{\tau\phi} = Q_{\phi\tau}\). We can thus calculate the Cholesky
decomposition

\[
L_\text{post} = \begin{bmatrix}
L_{11} & 0 & 0 \\
L_{21} & L_{22} & 0 \\
L_{31} & L_{32} & L_{33},
\end{bmatrix}
\]

efficiently using a block-Cholesky factorization:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\tightlist
\item
  \(L_{11} = \mathrm{Chol}(Q_{\psi\psi} + \tau_\psi Q_\text{prior})\)
\item
  \(L_{21} = Q_{\psi\tau}L_{11}^{-T}\)
\item
  \(L_{31} = Q_{\psi\phi}L_{11}^{-T}\)
\item
  \(L_{22} = \mathrm{Chol}(S_{22})\)

  \begin{itemize}
  \tightlist
  \item
    \(S_{22} = \left(Q_{\tau\tau} + \tau_\tau Q_\text{prior}\right) - L_{21}L_{21}^T\)
  \end{itemize}
\item
  \(L_{32} = \left(Q_{\tau\phi} - L_{31}L_{21}^T\right)L_{22}^{-T}\)
\item
  \(L_{33} = \mathrm{Chol}(S_{33})\)

  \begin{itemize}
  \tightlist
  \item
    \(S_{33} = \left( Q_{\phi\phi} + \tau_\phi Q_\text{prior} \right) - L_{31}L_{31}^T - L_{32}L_{32}^T\)
  \end{itemize}
\end{enumerate}

\subsection{\texorpdfstring{Forward Substitution: Solving
\(Ly = b\)}{Forward Substitution: Solving Ly = b}}\label{forward-substitution-solving-ly-b}

Partition \(L\), \(y\) and \(b\) conformably:

\[
L_\text{post} = \begin{bmatrix}
L_{11} & 0 & 0 \\
L_{21} & L_{22} & 0 \\
L_{31} & L_{32} & L_{33},
\end{bmatrix} \qquad
y = \begin{bmatrix}
y_1 \\
y_2 \\
y_3
\end{bmatrix} \qquad
b = \begin{bmatrix}
b_1 \\
b_2 \\
b_3
\end{bmatrix}.
\]

\subsubsection{\texorpdfstring{\textbf{First Block
Row}}{First Block Row}}\label{first-block-row}

\[
L_{11} y_1 = b_1
\]

Solve for \(y_1\):

\[
y_1 = L_{11}^{-1}b_1
\]

\subsubsection{\texorpdfstring{\textbf{Second Block
Row}}{Second Block Row}}\label{second-block-row}

\[
L_{21}y_1 + L_{22}y_2 = b_2
\]

Rearranged:

\[
L_{22}y_2 = b_2 - L_{21}y_1
\]

Solve for \(y_2\):

\[
y_2 = L_{22}^{-1}\left(b_2 - L_{21}y_1\right)
\]

\subsubsection{\texorpdfstring{\textbf{Third Block
Row}}{Third Block Row}}\label{third-block-row}

\[
L_{31}y_1 + L_{32}y_2 + L_{33}y_3 = b_3
\]

Rearranged:

\[
L_{33}y_3 = b_3 - L_{31}y_1 - L_{32}y_2
\]

Solve for \(y_3\):

\[
y_3 = L_{33}^{-1}\left(b_3 - L_{31}y_1 - L_{32}y_2\right)
\]

\subsubsection{\texorpdfstring{\textbf{Combined
Solution}}{Combined Solution}}\label{combined-solution}

\[
y = \begin{bmatrix}
y_1 \\
y_2 \\
y_3
\end{bmatrix} =
\begin{bmatrix}
 L_{11}^{-1}b_1 \\
L_{22}^{-1}\left(b_2 - L_{21}y_1\right) \\
L_{33}^{-1}\left(b_3 - L_{31}y_1 - L_{32}y_2\right)
\end{bmatrix}
\]

\subsection{\texorpdfstring{Backward Substitution: Solving
\(L^Tx = y\)}{Backward Substitution: Solving L\^{}Tx = y}}\label{backward-substitution-solving-ltx-y}

Partition \(L^T\) and \(x\) like \(y\):

\[
L^T = \begin{bmatrix}
L_{11}^T & L_{21}^T & L_{31}^T \\
0 & L_{22}^T & L_{32}^T \\
0 & 0 & L_{33}^T
\end{bmatrix}, \quad
x = \begin{bmatrix}
x_1 \\
x_2 \\
x_3
\end{bmatrix}
\]

\subsubsection{\texorpdfstring{\textbf{Third Block
Row}}{Third Block Row}}\label{third-block-row-1}

\[
L_{33}^Tx_3 = y_3
\]

Solve for \(x_3\)

\[
x_3 = L_{33}^{-T}y_3
\]

\subsubsection{\texorpdfstring{\textbf{Second Block
Row}}{Second Block Row}}\label{second-block-row-1}

\[
L_{22}^Tx_2 + L_{32}^Tx_3 = y_2
\]

Rearranged:

\[
L_{22}^Tx_2 = y_2 - L_{32}^Tx_3
\]

Solve for \(x_2\)

\[
x_2 = L_{22}^{-T}\left(y_2 - L_{32}^Tx_3\right)
\]

\subsubsection{\texorpdfstring{\textbf{First Block
Row}}{First Block Row}}\label{first-block-row-1}

\[
L_{11}^Tx_1 + L_{21}^Tx_2 + L_{31}^Tx_3 = y_1
\]

Rearranged:

\[
L_{11}^Tx_1 = y_1 - L_{21}^Tx_2 - L_{31}^Tx_3
\]

Solve for \(x_1\):

\[
x_1 = L_{11}^{-T}\left(y_1 - L_{21}^Tx_2 - L_{31}^Tx_3\right)
\]

\subsubsection{\texorpdfstring{\textbf{Combined
Solution}}{Combined Solution}}\label{combined-solution-1}

\[
x = \begin{bmatrix}
x_1 \\
x_2 \\
x_3
\end{bmatrix} =
\begin{bmatrix}
L_{11}^{-T}\left(y_1 - L_{21}^Tx_2 - L_{31}^Tx_3\right) \\
L_{22}^{-T}\left(y_2 - L_{32}^Tx_3\right) \\
L_{33}^{-T}y_3
\end{bmatrix}
\]

\subsection{Summary of Forward-Backward
Step}\label{summary-of-forward-backward-step}

First solve \(Ly = b\):

\[
y = \begin{bmatrix}
y_1 \\
y_2 \\
y_3
\end{bmatrix} =
\begin{bmatrix}
 L_{11}^{-1}b_1 \\
L_{22}^{-1}\left(b_2 - L_{21}y_1\right) \\
L_{33}^{-1}\left(b_3 - L_{31}y_1 - L_{32}y_2\right)
\end{bmatrix}
\]

Then solve \(L^Tx = y\)

\[
x = \begin{bmatrix}
x_1 \\
x_2 \\
x_3
\end{bmatrix} =
\begin{bmatrix}
L_{11}^{-T}\left(y_1 - L_{21}^Tx_2 - L_{31}^Tx_3\right) \\
L_{22}^{-T}\left(y_2 - L_{32}^Tx_3\right) \\
L_{33}^{-T}y_3
\end{bmatrix}
\]




\end{document}
